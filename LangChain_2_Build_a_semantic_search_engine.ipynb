{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOU+iM9cPo/Ug04GFJNMEJN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sampathk-hps/langchain-fundamentals-colab/blob/main/LangChain_2_Build_a_semantic_search_engine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LangChain's document loader, embedding, and vector store abstractions.\n",
        "These abstractions are designed to support retrieval of data-- from (vector) databases and other sources-- for integration with LLM workflows. They are important for applications that fetch data to be reasoned over as part of model inference, as in the case of retrieval-augmented generation, or RAG"
      ],
      "metadata": {
        "id": "kng8q-FcZUTv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Concepts\n",
        "\n",
        "* Documents and document loaders\n",
        "* Text splitters;\n",
        "* Embeddings;\n",
        "* Vector stores and retrievers.\n"
      ],
      "metadata": {
        "id": "cuKPUqSpZmcA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation"
      ],
      "metadata": {
        "id": "m6ySgyxSaEX5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "EiOyuEj4Y0IY",
        "outputId": "af12694c-b7d6-42b6-d242-ded2282f5de3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.30-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-6.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=0.3.75 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.3.76)\n",
            "Requirement already satisfied: langchain<2.0.0,>=0.3.27 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.3.27)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.43)\n",
            "Collecting requests<3.0.0,>=2.32.5 (from langchain-community)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.10.1)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.28)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain<2.0.0,>=0.3.27->langchain-community) (0.3.11)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain<2.0.0,>=0.3.27->langchain-community) (2.11.9)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain-community) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain-community) (4.15.0)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain-community) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.25.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.2.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<2.0.0,>=0.3.75->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain-community) (2.33.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.3.1)\n",
            "Downloading langchain_community-0.3.30-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-6.1.1-py3-none-any.whl (323 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.5/323.5 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: requests, pypdf, mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain-community\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.6.7 langchain-community-0.3.30 marshmallow-3.26.1 mypy-extensions-1.1.0 pypdf-6.1.1 requests-2.32.5 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "pip install langchain-community pypdf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "try:\n",
        "  from dotenv import load_dotenv\n",
        "  load_dotenv()\n",
        "except ImportError:\n",
        "  pass\n",
        "\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "if \"LANGSMITH_API_KEY\" not in os.environ:\n",
        "    os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\n",
        "        prompt=\"Enter your LangSmith API key (optional): \"\n",
        "    )\n",
        "if \"LANGSMITH_PROJECT\" not in os.environ:\n",
        "    os.environ[\"LANGSMITH_PROJECT\"] = getpass.getpass(\n",
        "        prompt='Enter your LangSmith Project Name (default = \"default\"): '\n",
        "    )\n",
        "    if not os.environ.get(\"LANGSMITH_PROJECT\"):\n",
        "        os.environ[\"LANGSMITH_PROJECT\"] = \"default\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqfV1xO-aVbf",
        "outputId": "39d261ea-f3a0-4fe2-8608-58bda4713aae"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your LangSmith API key (optional): ··········\n",
            "Enter your LangSmith Project Name (default = \"default\"): ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Documents and Document Loaders\n",
        "\n",
        "LangChain implements a Document abstraction, which is intended to represent a unit of text and associated metadata. It has three attributes:\n",
        "\n",
        "1. page_content: a string representing the content;\n",
        "2. metadata: a dict containing arbitrary metadata;\n",
        "3. id: (optional) a string identifier for the document.\n",
        "\n",
        "The metadata attribute can capture information about the source of the document, its relationship to other documents, and other information.\n",
        "\n",
        "Individual Document object often represents a **chunk** of a larger document."
      ],
      "metadata": {
        "id": "7F2RAXysaylv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We can generate sample documents when desired:\n",
        "\n",
        "\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "documents = [\n",
        "    Document(\n",
        "        page_content=\"Dogs are great companions, known for their loyalty and friendliness.\",\n",
        "        metadata={\"source\": \"mammal-pets-doc\"},\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Cats are independent pets that often enjoy their own space.\",\n",
        "        metadata={\"source\": \"mammal-pets-doc\"},\n",
        "    ),\n",
        "]"
      ],
      "metadata": {
        "id": "VZIy3ZysavpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU langchain-community bs4"
      ],
      "metadata": {
        "id": "8sk1ZEkpvJMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import BSHTMLLoader\n",
        "\n",
        "# loader = PyPDFLoader(\"data/state_of_the_union.pdf\")\n",
        "# documents = loader.load()\n",
        "# loader = CSVLoader(\n",
        "#     file_path=\"/content/sample_data/california_housing_train.csv\")\n",
        "loader = BSHTMLLoader(\n",
        "    file_path=\"/content/sample_data/2023-03-28-TSM-BZ$14109a48.html\")\n",
        "docs = loader.load()\n",
        "print(len(docs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TipEJvkIb744",
        "outputId": "7e9aa166-67de-4ab5-aa1d-aa806bcb24ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"{docs[0].page_content[:200]}\\n\")\n",
        "print(docs[0].metadata)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6B7cIzscxNr",
        "outputId": "d61eefba-6e53-44e9-b7ec-766aa2d2fe4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "U.S. advanced semiconductor technology embargo on China and ramp-up of the semiconductor technology base have affected China.  Shares in top Chinese chipmakers shed $7.7 billion in market value on Oct\n",
            "\n",
            "{'source': '/content/sample_data/2023-03-28-TSM-BZ$14109a48.html', 'title': ''}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splitting"
      ],
      "metadata": {
        "id": "mtNOq6yjc8Gb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, add_start_index=True)\n",
        "all_splits = text_splitter.split_documents(docs)\n",
        "len(all_splits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I54nK3Jmc93L",
        "outputId": "b3f9149f-88d5-4481-9a44-6e5d5922e3d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embeddings"
      ],
      "metadata": {
        "id": "9Z9oVAgdd8nV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using **HuggingFace** Embedding Model"
      ],
      "metadata": {
        "id": "g2Q2_2jeeYCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -qU langchain-huggingface"
      ],
      "metadata": {
        "id": "BxuAGTvgd5jW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
      ],
      "metadata": {
        "id": "RaJofdYJeeZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_1 = embeddings.embed_query(all_splits[0].page_content)\n",
        "vector_2 = embeddings.embed_query(all_splits[1].page_content)\n",
        "\n",
        "assert len(vector_1) == len(vector_2)\n",
        "print(f\"Generated vectors of length {len(vector_1)}\\n\")\n",
        "print(vector_1[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJ8g71xVeipQ",
        "outputId": "a94a0763-5c96-483b-fcc2-89447115b254"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated vectors of length 768\n",
            "\n",
            "[-0.02542412094771862, 0.03560704365372658, -0.030742721632122993, -0.044747497886419296, 0.04758850485086441, -0.04024335369467735, 0.061291590332984924, 0.031164580956101418, 0.007982350885868073, 0.014209233224391937]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using **Ollama** Embedding Model"
      ],
      "metadata": {
        "id": "JicHHUL8evd-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -qU langchain-ollama"
      ],
      "metadata": {
        "id": "k4oENbYje-13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_ollama import OllamaEmbeddings\n",
        "\n",
        "embeddings = OllamaEmbeddings(model=\"llama3\")"
      ],
      "metadata": {
        "id": "zCdzABVofKOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_1 = embeddings.embed_query(all_splits[0].page_content)\n",
        "vector_2 = embeddings.embed_query(all_splits[1].page_content)\n",
        "\n",
        "assert len(vector_1) == len(vector_2)\n",
        "print(f\"Generated vectors of length {len(vector_1)}\\n\")\n",
        "print(vector_1[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "collapsed": true,
        "id": "96P7JS4FfVL5",
        "outputId": "1dd88b65-0012-4c2f-ea7d-b71292e5a92b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ConnectionError",
          "evalue": "Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2894162498.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvector_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_splits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvector_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_splits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector_1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Generated vectors of length {len(vector_1)}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_ollama/embeddings.py\u001b[0m in \u001b[0;36membed_query\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0membed_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0;34m\"\"\"Embed query text.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0maembed_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_ollama/embeddings.py\u001b[0m in \u001b[0;36membed_documents\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    300\u001b[0m             )\n\u001b[1;32m    301\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m         return self._client.embed(\n\u001b[0m\u001b[1;32m    303\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_alive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeep_alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         )[\"embeddings\"]\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ollama/_client.py\u001b[0m in \u001b[0;36membed\u001b[0;34m(self, model, input, truncate, options, keep_alive, dimensions)\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[0mdimensions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m   ) -> EmbedResponse:\n\u001b[0;32m--> 377\u001b[0;31m     return self._request(\n\u001b[0m\u001b[1;32m    378\u001b[0m       \u001b[0mEmbedResponse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m       \u001b[0;34m'POST'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ollama/_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cls, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ollama/_client.py\u001b[0m in \u001b[0;36m_request_raw\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConnectError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONNECTION_ERROR_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mConnectionError\u001b[0m: Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Google Vertex"
      ],
      "metadata": {
        "id": "cskakZRomF7O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -qU langchain-google-vertexai"
      ],
      "metadata": {
        "id": "6qlgDZanmJpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_vertexai import VertexAIEmbeddings\n",
        "\n",
        "embeddings = VertexAIEmbeddings(model=\"text-embedding-005\", project=\"your-gcp-project-id\")"
      ],
      "metadata": {
        "id": "IZtEBTNXmSlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_1 = embeddings.embed_query(all_splits[0].page_content)\n",
        "vector_2 = embeddings.embed_query(all_splits[1].page_content)\n",
        "\n",
        "assert len(vector_1) == len(vector_2)\n",
        "print(f\"Generated vectors of length {len(vector_1)}\\n\")\n",
        "print(vector_1[:10])"
      ],
      "metadata": {
        "id": "DgYmWrCJmYha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vector stores"
      ],
      "metadata": {
        "id": "sCV29JDBm6xW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Memory"
      ],
      "metadata": {
        "id": "GCuw3ThRnJE5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -qU langchain-core"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JO5Rjwu0m7xP",
        "outputId": "16fb43f0-3990-4a77-a4dd-a590a6befd06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/449.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m317.4/449.5 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m449.5/449.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.vectorstores import InMemoryVectorStore\n",
        "\n",
        "vector_store = InMemoryVectorStore(embeddings)"
      ],
      "metadata": {
        "id": "WpvJdZzDnOGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Having instantiated our vector store, we can now index the documents.\n",
        "\n",
        "ids = vector_store.add_documents(documents=all_splits)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "D2pbNaJcnmF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using ChromaDB"
      ],
      "metadata": {
        "id": "7lm3Ix11oJ1S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -qU langchain-chroma"
      ],
      "metadata": {
        "id": "5cJPxmyPoM07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5b52c3ca"
      },
      "source": [
        "!rm -rf ./chroma_langchain_db"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_chroma import Chroma\n",
        "\n",
        "vector_store = Chroma(collection_name=\"example_html_collection\", embedding_function=embeddings, persist_directory=\"./chroma_langchain_html_db\",)"
      ],
      "metadata": {
        "id": "8Htibi5moQeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = vector_store.add_documents(documents=all_splits)"
      ],
      "metadata": {
        "id": "Rhysazxsoj2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using FAISS - Facebook AI Similarity Search"
      ],
      "metadata": {
        "id": "ca1CMyB0pAYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -qU langchain-community"
      ],
      "metadata": {
        "id": "hjipnYwbpErs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfefef6a",
        "outputId": "55e11350-8dc0-4c9a-ba5f-e2bfefbde7c1"
      },
      "source": [
        "pip install -qU faiss-cpu"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
        "from langchain_community.vectorstores.faiss import FAISS\n",
        "\n",
        "embedding_dim = len(embeddings.embed_query(\"hello world\"))\n",
        "index = faiss.IndexFlatL2(embedding_dim)\n",
        "\n",
        "vector_store = FAISS(\n",
        "    embedding_function=embeddings,\n",
        "    index=index,\n",
        "    docstore=InMemoryDocstore(),\n",
        "    index_to_docstore_id={},\n",
        ")"
      ],
      "metadata": {
        "id": "S9cYZcM8pG6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = vector_store.add_documents(documents=all_splits)"
      ],
      "metadata": {
        "id": "4bSbSYJtqASH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Usage"
      ],
      "metadata": {
        "id": "ZOOLU_v6qN9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Return documents based on similarity to a string query:\n",
        "\n",
        "results = vector_store.similarity_search(\n",
        "    \"How many distribution centers does Nike have in the US?\"\n",
        ")\n",
        "\n",
        "print(results[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPzeA5leqPUN",
        "outputId": "771fc315-bc2d-4ba0-ab2b-cfbb32d87c5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='KraneShares Trust KraneShares CSI China Internet ETF (NYSE: KWEB), and iShares MSCI China ETF (NASDAQ: MCHI) gained between 0.4%- 2.7% YTD.  The Chinese ETFs have significant exposure to Tencent Holding Ltd (OTC: TCEHY), Alibaba Group Holding Limited (NYSE: BABA), Baidu, Inc (NASDAQ: BIDU), JD.Com, Inc (NASDAQ: JD), and more.  Other factors, such as China's crackdown on tech, also impacted major players like Alibaba, JD, and Baidu.  Photo by Tatiana Popova and rawf8 via Shuttterstock  Copyright © Benzinga. All rights reserved. Write to editorial@benzinga.com with any questions about this content. Benzinga does not provide investment advice.' metadata={'source': '/content/sample_data/2023-03-28-TSM-BZ$14109a48.html', 'title': '', 'start_index': 1617}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Async Query\n",
        "\n",
        "results = await vector_store.asimilarity_search(\"When was Nike incorporated?\")\n",
        "\n",
        "print(results[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_IULqixsGhU",
        "outputId": "0fd6cfe5-3af4-42b5-eec2-0e8f65cd6bed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='KraneShares Trust KraneShares CSI China Internet ETF (NYSE: KWEB), and iShares MSCI China ETF (NASDAQ: MCHI) gained between 0.4%- 2.7% YTD.  The Chinese ETFs have significant exposure to Tencent Holding Ltd (OTC: TCEHY), Alibaba Group Holding Limited (NYSE: BABA), Baidu, Inc (NASDAQ: BIDU), JD.Com, Inc (NASDAQ: JD), and more.  Other factors, such as China's crackdown on tech, also impacted major players like Alibaba, JD, and Baidu.  Photo by Tatiana Popova and rawf8 via Shuttterstock  Copyright © Benzinga. All rights reserved. Write to editorial@benzinga.com with any questions about this content. Benzinga does not provide investment advice.' metadata={'source': '/content/sample_data/2023-03-28-TSM-BZ$14109a48.html', 'title': '', 'start_index': 1617}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Return Scores:\n",
        "# Note that providers implement different scores; the score here\n",
        "# is a distance metric that varies inversely with similarity.\n",
        "\n",
        "results = vector_store.similarity_search_with_score(\"What was Nike's revenue in 2023?\")\n",
        "doc, score = results[0]\n",
        "print(f\"Score: {score}\\n\")\n",
        "print(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "folBBLEosS53",
        "outputId": "52f7d8a1-8795-49fa-f63e-ec5b314a4483"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score: 1.644883632659912\n",
            "\n",
            "page_content='vying for funding cannot increase their production of advanced chips in China.  Leading U.S. tech ETFs SPDR Select Sector Fund - Technology (NYSE:XLK), VanEck Semiconductor ETF (NASDAQ:SMH), and iShares Semiconductor ETF (NASDAQ:SOXX) have gained between 17.6% - 25.5% year-to-date.  SMH and SOXX with exposure to Nvidia Corp (NASDAQ: NVDA), Taiwan Semiconductor Manufacturing Company Ltd (NYSE: TSM), Advanced Micro Devices, Inc (NASDAQ: AMD), ASML Holding N.V. (NASDAQ: ASML), Texas Instruments Inc (NASDAQ: TXN), Intel Corp (NASDAQ: INTC) and other chipmakers led the gains.  XLK has significant exposure to Microsoft Corp (NASDAQ: MSFT), Apple Inc (NASDAQ: AAPL), followed by chipmakers Nvidia and more.  Contrastingly leading Chinese tech ETF gains trailed U.S. peers. iShares China Large-Cap ETF (NYSE: FXI), KraneShares Trust KraneShares CSI China Internet ETF (NYSE: KWEB), and iShares MSCI China ETF (NASDAQ: MCHI) gained between 0.4%- 2.7% YTD.  The Chinese ETFs have significant exposure' metadata={'source': '/content/sample_data/2023-03-28-TSM-BZ$14109a48.html', 'title': '', 'start_index': 802}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Return documents based on similarity to an embedded query:\n",
        "embedding = embeddings.embed_query(\"How were Nike's margins impacted in 2023?\")\n",
        "\n",
        "results = vector_store.similarity_search_by_vector(embedding)\n",
        "print(results[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNCokcn-spQw",
        "outputId": "bdc59502-8b97-4671-9a7b-173d44ba1c2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='vying for funding cannot increase their production of advanced chips in China.  Leading U.S. tech ETFs SPDR Select Sector Fund - Technology (NYSE:XLK), VanEck Semiconductor ETF (NASDAQ:SMH), and iShares Semiconductor ETF (NASDAQ:SOXX) have gained between 17.6% - 25.5% year-to-date.  SMH and SOXX with exposure to Nvidia Corp (NASDAQ: NVDA), Taiwan Semiconductor Manufacturing Company Ltd (NYSE: TSM), Advanced Micro Devices, Inc (NASDAQ: AMD), ASML Holding N.V. (NASDAQ: ASML), Texas Instruments Inc (NASDAQ: TXN), Intel Corp (NASDAQ: INTC) and other chipmakers led the gains.  XLK has significant exposure to Microsoft Corp (NASDAQ: MSFT), Apple Inc (NASDAQ: AAPL), followed by chipmakers Nvidia and more.  Contrastingly leading Chinese tech ETF gains trailed U.S. peers. iShares China Large-Cap ETF (NYSE: FXI), KraneShares Trust KraneShares CSI China Internet ETF (NYSE: KWEB), and iShares MSCI China ETF (NASDAQ: MCHI) gained between 0.4%- 2.7% YTD.  The Chinese ETFs have significant exposure' metadata={'source': '/content/sample_data/2023-03-28-TSM-BZ$14109a48.html', 'title': '', 'start_index': 802}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrivers"
      ],
      "metadata": {
        "id": "3uv7IOJA1BFv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.runnables import chain\n",
        "\n",
        "\n",
        "@chain\n",
        "def retriever(query: str) -> List[Document]:\n",
        "    return vector_store.similarity_search(query, k=1)\n",
        "\n",
        "\n",
        "retriever.batch(\n",
        "    [\n",
        "        \"How many distribution centers does Nike have in the US?\",\n",
        "        \"When was Nike incorporated?\",\n",
        "    ],\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "y1owXuFI1DYN",
        "outputId": "3c4613e2-696f-4025-8dab-6fe7b5de316f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[Document(id='73c498e4-1c5d-43e6-a8be-8f62f717a1fa', metadata={'source': '/content/sample_data/2023-03-28-TSM-BZ$14109a48.html', 'title': '', 'start_index': 1617}, page_content=\"KraneShares\\xa0Trust KraneShares CSI China Internet ETF\\xa0(NYSE:\\xa0KWEB), and iShares MSCI China ETF\\xa0(NASDAQ:\\xa0MCHI) gained between 0.4%- 2.7% YTD.  The Chinese ETFs have significant exposure to\\xa0Tencent Holding Ltd\\xa0(OTC:\\xa0TCEHY),\\xa0Alibaba Group Holding Limited\\xa0(NYSE:\\xa0BABA),\\xa0Baidu, Inc\\xa0(NASDAQ:\\xa0BIDU),\\xa0JD.Com, Inc\\xa0(NASDAQ:\\xa0JD), and more.  Other factors, such as China's crackdown on tech, also impacted major players like Alibaba, JD, and Baidu.  Photo by Tatiana Popova and rawf8 via Shuttterstock  Copyright © Benzinga. All rights reserved. Write to editorial@benzinga.com with any questions about this content. Benzinga does not provide investment advice.\")],\n",
              " [Document(id='73c498e4-1c5d-43e6-a8be-8f62f717a1fa', metadata={'source': '/content/sample_data/2023-03-28-TSM-BZ$14109a48.html', 'title': '', 'start_index': 1617}, page_content=\"KraneShares\\xa0Trust KraneShares CSI China Internet ETF\\xa0(NYSE:\\xa0KWEB), and iShares MSCI China ETF\\xa0(NASDAQ:\\xa0MCHI) gained between 0.4%- 2.7% YTD.  The Chinese ETFs have significant exposure to\\xa0Tencent Holding Ltd\\xa0(OTC:\\xa0TCEHY),\\xa0Alibaba Group Holding Limited\\xa0(NYSE:\\xa0BABA),\\xa0Baidu, Inc\\xa0(NASDAQ:\\xa0BIDU),\\xa0JD.Com, Inc\\xa0(NASDAQ:\\xa0JD), and more.  Other factors, such as China's crackdown on tech, also impacted major players like Alibaba, JD, and Baidu.  Photo by Tatiana Popova and rawf8 via Shuttterstock  Copyright © Benzinga. All rights reserved. Write to editorial@benzinga.com with any questions about this content. Benzinga does not provide investment advice.\")]]"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vectorstores implement an as_retriever method that will generate a Retriever, specifically a VectorStoreRetriever. These retrievers include specific search_type and search_kwargs attributes that identify what methods of the underlying vector store to call, and how to parameterize them. For instance, we can replicate the above with the following:"
      ],
      "metadata": {
        "id": "yk3xPbnB1rqz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vector_store.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\"k\": 1},\n",
        ")\n",
        "\n",
        "retriever.batch(\n",
        "    [\n",
        "        \"How many distribution centers does Nike have in the US?\",\n",
        "        \"When was Nike incorporated?\",\n",
        "    ],\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAVDAYeu1eWf",
        "outputId": "e1f3edd4-3125-44ee-aced-7dd65ed1a107"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[Document(id='73c498e4-1c5d-43e6-a8be-8f62f717a1fa', metadata={'source': '/content/sample_data/2023-03-28-TSM-BZ$14109a48.html', 'title': '', 'start_index': 1617}, page_content=\"KraneShares\\xa0Trust KraneShares CSI China Internet ETF\\xa0(NYSE:\\xa0KWEB), and iShares MSCI China ETF\\xa0(NASDAQ:\\xa0MCHI) gained between 0.4%- 2.7% YTD.  The Chinese ETFs have significant exposure to\\xa0Tencent Holding Ltd\\xa0(OTC:\\xa0TCEHY),\\xa0Alibaba Group Holding Limited\\xa0(NYSE:\\xa0BABA),\\xa0Baidu, Inc\\xa0(NASDAQ:\\xa0BIDU),\\xa0JD.Com, Inc\\xa0(NASDAQ:\\xa0JD), and more.  Other factors, such as China's crackdown on tech, also impacted major players like Alibaba, JD, and Baidu.  Photo by Tatiana Popova and rawf8 via Shuttterstock  Copyright © Benzinga. All rights reserved. Write to editorial@benzinga.com with any questions about this content. Benzinga does not provide investment advice.\")],\n",
              " [Document(id='73c498e4-1c5d-43e6-a8be-8f62f717a1fa', metadata={'source': '/content/sample_data/2023-03-28-TSM-BZ$14109a48.html', 'title': '', 'start_index': 1617}, page_content=\"KraneShares\\xa0Trust KraneShares CSI China Internet ETF\\xa0(NYSE:\\xa0KWEB), and iShares MSCI China ETF\\xa0(NASDAQ:\\xa0MCHI) gained between 0.4%- 2.7% YTD.  The Chinese ETFs have significant exposure to\\xa0Tencent Holding Ltd\\xa0(OTC:\\xa0TCEHY),\\xa0Alibaba Group Holding Limited\\xa0(NYSE:\\xa0BABA),\\xa0Baidu, Inc\\xa0(NASDAQ:\\xa0BIDU),\\xa0JD.Com, Inc\\xa0(NASDAQ:\\xa0JD), and more.  Other factors, such as China's crackdown on tech, also impacted major players like Alibaba, JD, and Baidu.  Photo by Tatiana Popova and rawf8 via Shuttterstock  Copyright © Benzinga. All rights reserved. Write to editorial@benzinga.com with any questions about this content. Benzinga does not provide investment advice.\")]]"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    }
  ]
}